{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import urllib.request\n",
    "import re\n",
    "kanji_list = r'[㐀-䶵一-鿋豈-頻,々]'\n",
    "ascii_char = r'[ -~]'\n",
    "hiragana_full = r'[ぁ-ゟ]'\n",
    "katakana_full = r'[=-ヿ]'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1692846306890"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read \"add_words.txt\" and return a list of words without the newline character\n",
    "def read_words():\n",
    "    with open(\"add_words.txt\", \"r\") as f:\n",
    "        words = f.readlines()\n",
    "    words = [x.strip() for x in words if x.strip() != \"\"]\n",
    "    return words\n",
    "\n",
    "# source: https://github.com/olsgaard/Japanese_nlp_scripts/blob/master/jp_regex.py\n",
    "# Defining necessary functions #\n",
    "\n",
    "def request(action, **params):\n",
    "    return {'action': action, 'params': params, 'version': 6}\n",
    "\n",
    "\n",
    "def invoke(action, **params):\n",
    "    requestJson = json.dumps(request(action, **params)).encode('utf-8')\n",
    "    response = json.load(urllib.request.urlopen(urllib.request.Request('http://127.0.0.1:8765', requestJson)))\n",
    "    if len(response) != 2:\n",
    "        raise Exception('response has an unexpected number of fields')\n",
    "    if 'error' not in response:\n",
    "        raise Exception('response is missing required error field')\n",
    "    if 'result' not in response:\n",
    "        raise Exception('response is missing required result field')\n",
    "    if response['error'] is not None:\n",
    "        raise Exception(response['error'])\n",
    "    return response['result']\n",
    "\n",
    "\n",
    "def remove_unicode_block(unicode_block, string):    # remove all characters from a unicode block\n",
    "    return re.sub(unicode_block, '', string)\n",
    "\n",
    "def extract_unicode_block(unicode_block, string):   # extract all characters from a unicode block\n",
    "    return re.findall(unicode_block, string)\n",
    "\n",
    "\n",
    "invoke('createDeck', deck='Core 2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression = ''        # この文は例えばです\n",
    "reading = ''          # この文[ぶん]は例えば[たとえば]です\n",
    "sentence_kana = ''   # このぶんはたとえばです\n",
    "sentence_en = ''    # This sentence is an example\n",
    "\n",
    "\n",
    "def get_japanese_sentence(kanji):\n",
    "    linkpage = f'https://jisho.org/search/{kanji}%20%23sentences'\n",
    "    page = requests.get(linkpage, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    sentences = soup.find_all('div', class_='sentence_content')\n",
    "    sentence_size = 1e9\n",
    "    best_sentence_length = 1    # choose the shortest sentence\n",
    "\n",
    "    if len(sentences) == 0:\n",
    "        return '', '', '', ''\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        japanese_all_letters = sentences[i].find('ul', class_='japanese_sentence japanese japanese_gothic clearfix')\n",
    "        expression_possible = ''   # この文は例えばです\n",
    "        reading_possible = ''      # この文[ぶん]は例えば[たとえば]です\n",
    "        sentence_kana_possible = ''    # このぶんはたとえばです\n",
    "        sentence_en_possible = sentences[i].find('div', class_='english_sentence clearfix').find('span', class_='english').get_text()\n",
    "\n",
    "        for count, l in enumerate(japanese_all_letters):\n",
    "            if type(l) is bs4.element.NavigableString:  # Se for uma string solta # if goes wrong change is to ==\n",
    "                expression_possible += l\n",
    "                reading_possible += l\n",
    "                sentence_kana_possible += l\n",
    "            elif type(l) is bs4.element.Tag:   # Se for um kanji com leitura # if goes wrong change is to ==\n",
    "                if l.find('span', class_='furigana') != None:   # Se tiver furigana\n",
    "                    furigana = l.find('span', class_='furigana').get_text()\n",
    "                    word = l.find('span', class_='unlinked').get_text()\n",
    "                    expression_possible += word\n",
    "                    sentence_kana_possible += furigana\n",
    "                    if count == 0:  # Unless it's the first word (?)\n",
    "                        reading_possible += word + '[' + furigana + ']'\n",
    "                    else:   # for some reason, there must be a space before the kanji with furigana for Anki to display it correctly\n",
    "                        # reading_possible += ' ' + word + '[' + furigana + ']'\n",
    "                        reading_possible += furigana_parser_sentences(word, furigana)\n",
    "                else:\n",
    "                    word = l.find('span', class_='unlinked').get_text()\n",
    "                    expression_possible += word\n",
    "                    reading_possible += word\n",
    "                    sentence_kana_possible += word\n",
    "\n",
    "        if kanji not in expression_possible:   # se a palavra não estiver na sentença, pular\n",
    "            continue\n",
    "\n",
    "        if abs(len(expression_possible) - best_sentence_length) < sentence_size:   # escolher a sentença mais próxima do tamanho desejado\n",
    "            expression = expression_possible\n",
    "            reading = reading_possible\n",
    "            sentence_kana = sentence_kana_possible\n",
    "            sentence_en = sentence_en_possible\n",
    "            sentence_size = abs(len(expression_possible) - best_sentence_length)\n",
    "\n",
    "    return expression, reading, sentence_kana, sentence_en\n",
    "\n",
    "\n",
    "def get_definition(kanji):\n",
    "    link_definition = f'https://jisho.org/search/{kanji}'\n",
    "    page_definition = requests.get(link_definition, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup_definition = BeautifulSoup(page_definition.content, 'html.parser')\n",
    "    if soup_definition.find('div', class_='concept_light clearfix') == None:\n",
    "        print(f'It wasnt possible to find a definition for {kanji}')\n",
    "        return None, None\n",
    "\n",
    "    first_definition = soup_definition.find('div', class_='concept_light clearfix')\n",
    "    definition = first_definition.find('div', class_='meaning-definition zero-padding').find_all('span', class_='meaning-meaning')\n",
    "    various_definitions = first_definition.find_all('div', class_='meaning-wrapper')\n",
    "    leituras = first_definition.find_all('span', class_='furigana')\n",
    "    definition = definition[0].get_text().split(';')\n",
    "\n",
    "    # place furigana in a list, for the case of words with separated kanjis\n",
    "    furigana_separados = []\n",
    "    for l in leituras:\n",
    "        x = l.find_all('span', class_='kanji')\n",
    "        if len(x) > 0:\n",
    "            for j in range(len(x)):\n",
    "                furigana_separados.append(x[j].get_text())\n",
    "\n",
    "    leitura = \"\"\n",
    "    definitions = []\n",
    "    definitions_new = ''\n",
    "\n",
    "    i = 0\n",
    "    for defini in various_definitions:\n",
    "        if defini.find('span', class_='meaning-meaning') == None:\n",
    "            break\n",
    "        definitions_new += defini.find('span', class_='meaning-meaning').get_text() + '<br> '\n",
    "        i += 1\n",
    "        if i == 3:\n",
    "            break\n",
    "    definitions_new = definitions_new[:-2]\n",
    "\n",
    "    for d in definition:\n",
    "        definitions.append(d.strip())\n",
    "        if len(definitions) == 3:\n",
    "            break\n",
    "\n",
    "    for i in range(len(leituras)):\n",
    "        x = leituras[i].find_all('span', class_='kanji')\n",
    "        if len(x) > 0:\n",
    "            for j in range(len(x)):\n",
    "                leitura += x[j].get_text()\n",
    "\n",
    "    return furigana_separados, definitions_new#definitions\n",
    "\n",
    "\n",
    "# Format the word with furigana, for the case of words with separated kanjis, as in definitions\n",
    "# it returns the kanji with the specific furigana; e.g. 片[かた]付[づ]ける\n",
    "# and this can be very interesting for the case of words with kanjis separated by hiragana, e.g. 申し込む\n",
    "def furigana_parser(word, furigana):\n",
    "    # assign the first kanji to the first furigana, the second kanji to the second furigana, and so on\n",
    "    word_furigana = ''\n",
    "    for char in word:\n",
    "        if re.match(kanji_list, char) and len(furigana) > 0:\n",
    "            word_furigana += \" \" + char + '[' + furigana.pop(0) + ']'\n",
    "        else:\n",
    "            word_furigana += char\n",
    "    return word_furigana\n",
    "\n",
    "\n",
    "# Format the word with furigana, for the case of sentences, because in jisho the furiganas are not separated by kanji in the case of sentences\n",
    "# it returns everything together; e.g. 片付ける[かたづ]\n",
    "def furigana_parser_sentences(word, furigana):\n",
    "    word_furigana = ''\n",
    "    furigana = furigana.replace(' ', '')\n",
    "    word = word.replace(' ', '')\n",
    "    if len(furigana) == 0:\n",
    "        return word\n",
    "    if any(re.findall(kanji_list, word)):\n",
    "        first_kanji = re.search(kanji_list, word).group()\n",
    "        last_kanji = re.search(kanji_list, word[::-1]).group()[::-1]\n",
    "        first_kanji_index = word.index(first_kanji)\n",
    "        last_kanji_index = len(word) - word[::-1].index(last_kanji) - 1\n",
    "        word_furigana = \" \" + word[:first_kanji_index] + word[first_kanji_index:last_kanji_index+1] + \"[\" + furigana + \"]\" + word[last_kanji_index+1:]\n",
    "    else:\n",
    "        word_furigana = f\" {word}[{furigana}]\"\n",
    "    return word_furigana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot create note because it is a duplicate\n",
      "Error adding 誤魔化す to the deck Core 2000\n",
      "Added the word 登場人物 to deck Core 2000\n",
      "Added the word 共有 to deck Core 2000\n",
      "Added the word 美容院 to deck Core 2000\n",
      "Added the word 恐竜 to deck Core 2000\n",
      "Added the word 付箋 to deck Core 2000\n",
      "Added the word 関わり合い to deck Core 2000\n",
      "Added the word 暮らし振り to deck Core 2000\n",
      "Couldn't find a sentence for 自己肯定感, adding only the word\n",
      "Added the word 自己肯定感 to deck Core 2000\n"
     ]
    }
   ],
   "source": [
    "deck_name = \"Core 2000\"\n",
    "kanjis = read_words()\n",
    "for kanji in kanjis:\n",
    "    expression, reading, sentence_kana, sentence_en = get_japanese_sentence(kanji)\n",
    "    if expression == '':\n",
    "        print(f\"Couldn't find a sentence for {kanji}, adding only the word\")\n",
    "\n",
    "    leitura, definicoes = get_definition(kanji)\n",
    "\n",
    "    try:\n",
    "        invoke('addNote',\n",
    "            note={\n",
    "                \"deckName\": deck_name,\n",
    "                \"modelName\": \"Core 2000\",\n",
    "                \"fields\": {\n",
    "                    'Optimized-Voc-Index': kanji,\n",
    "                    'Vocabulary-Kanji': kanji,\n",
    "                    'Vocabulary-Kana': ''.join(leitura),\n",
    "                    'Vocabulary-Furigana' : furigana_parser(kanji, leitura),\n",
    "                    'Vocabulary-English': definicoes,\n",
    "                    'Expression': expression,\n",
    "                    'Reading': reading,\n",
    "                    'Sentence-Kana': sentence_kana,\n",
    "                    'Sentence-English': sentence_en,\n",
    "                },\n",
    "                \"options\": {\n",
    "                    \"allowDuplicate\": False,\n",
    "                    \"duplicateScope\": \"deck\",\n",
    "                    \"duplicateScopeOptions\": {\n",
    "                        \"deckName\": deck_name,\n",
    "                        \"checkChildren\": False,\n",
    "                        \"checkAllModels\": False\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\"Added the word \" + kanji + \" to deck \" + deck_name)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error adding \" + kanji + \" to the deck \" + deck_name)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
